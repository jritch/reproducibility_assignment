---
title: "Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

# Report Details

```{r}
articleID <- '2-10-2014' # insert the article ID code here e.g., "10-3-2015"
reportType <- 'copilot' # specify whether this is the 'pilot' report or 'copilot' report
pilotNames <- 'Jacob William Keith Ritchie' # insert the pilot's name here e.g., "Tom Hardwicke".
copilotNames <- 'Jackie Yang' # # insert the co-pilot's name here e.g., "Michael Frank".
pilotTTC <- 200  # insert the pilot's estimated time to complete (in minutes, it is fine to approximate) e.g., 120
copilotTTC <- 20 # insert the co-pilot's estimated time to complete (in minutes, it is fine to approximate) e.g., 120
pilotStartDate <- as.Date("11/06/19", format = "%m/%d/%y")   # insert the piloting start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("11/07/19", format = "%m/%d/%y") # insert the co-piloting start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("11/10/19", format = "%m/%d/%y") # insert the date of final report completion in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

This study uses a time capsule methodology. Undergraduate students answered questions describing their current experiences (social events, a question from a recent exam, a conversation) and were asked to predict how curious they would be about these experiences and how interesting they would find them (broken down into three dimensions: surprising, meaningful and interesting). Then, 3 months later they reviewed their responses and rated how curious they actually were about their answers, and how interesting they actually found them.

------

#### Target outcomes: 

For this article you should focus on the findings reported in the results section for Study 1 (and Table 1).

Specifically, you should attempt to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures:

> Table 1 provides descriptive statistics for each measure
for Study 1.

> Participantsâ€™ Time 1 predictions of their curiosity (M = 3.99, SD = 1.32) were lower than their actual curiosity ratings at Time 2, immediately before reading their responses (M = 4.34, SD = 1.25), t(105) = 2.88, p = .005, d = 0.27. Participants also underestimated how interesting they would find their responses. Predictions of interest at Time 1 (M = 3.54, SD = 1.01) were lower than ratings of actual interest experienced at Time 2 (M = 3.82, SD = 0.89), t(105) = 3.10, p = .003, d = 0.29.

**Note**
Make sure to use the original article for additional context and information about any necessary pre-processing steps. Also check for additional supplementary materials that may provide supporting documentation for analysis procedures.
------

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(ReproReports) # custom reporting functions
library(broom)
library(bootES)
library(effsize)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
data = read_sav("../data/Study1_Data.sav")
```

# Step 3: Tidy data

```{r}

data$id <- seq.int(nrow(data)) 

tidy_data <- data %>% 
  filter(T2_Finished == 1) %>% 
  select("id", contains("Interesting"), contains("Curious"), contains("Meaningful"), contains("Surprised"), -contains("diff"), -c("T1_Interesting", "T2_Interesting","T1_Curious", "T2_Curious","T1_Meaningful", "T2_Meaningful","T1_Surprised", "T2_Surprised")) %>%
  gather(measurement,score, -"id") %>% 
  mutate(time = sub("(.*)_.*_.*","\\1", measurement),
         item = sub(".*_(.*)_.*","\\1", measurement),
         measure = tolower(sub(".*_.*_(.*)","\\1", measurement)), # case is inconsistent, so make all lowercase
         interest_measure = factor(measure != "curious", , labels = c("curious", "interesting"))) 
```

# Step 4: Run analysis

## Pre-processing

There are five measures - curiosity, interest, meaningfulness, surprise and un-aggregated interest.

We take the mean of each measure across every question in the time capsule, and then across all the participants.

There are two measures that are both called interest. Aggregated interest is the mean of the three measures interest, surprise and un-aggregated interest. 
```{r}

aggregated_data = tidy_data %>%  group_by(id,time,interest_measure) %>% 
  summarise(avg_score = mean(score, na.rm = T)) %>% 
  spread(time,avg_score) %>%
  mutate(diff = (T2 - T1)) %>%
  group_by(interest_measure)  

# interest_measure column splits the data into curiosity and aggregated interest
curious_data <- aggregated_data %>% filter(interest_measure == "curious")

interesting_data <- aggregated_data %>% filter(interest_measure == "interesting")

# the measure column splits the data into the four un-aggregated measures (Curiosity, surprise, meaningfulness and interest )
un_aggregated_data = tidy_data %>%  group_by(id,time,measure) %>% 
  summarise(avg_score = mean(score, na.rm = T)) %>% 
  spread(time,avg_score) %>%
  mutate(diff = (T2 - T1)) %>%
  group_by(measure) 
  
surprising_data <- un_aggregated_data %>% filter(measure == "surprised")
meaningful_data <- un_aggregated_data %>% filter(measure == "meaningful")
un_aggregated_interesting_data <- un_aggregated_data %>% filter(measure == "interesting")

```

## Descriptive statistics
There is a large amount of repetition in this section, but we calculate means and 95% confidence intervals for all five measures at T1 and T2, and for the difference between T1 and T2, as reported in Table 1.

We also calculate standard deviations for the curiosity and aggregated interest measures, and cohen's d for the standard effect size of the difference between them, which are specified in the text.

The authors do not specify what method they used to calculate the 95% confidence intervals.

```{r}

boot_results = bootES(curious_data, R=10000, data.col="T1")
reproCheck('3.99', boot_results$t0, valueType="mean")
reproCheck('3.74', boot_results$bounds[1], valueType="ci")
reproCheck('4.24', boot_results$bounds[2], valueType="ci")

boot_results = bootES(curious_data, R=10000, data.col="T2")
reproCheck('4.34', boot_results$t0, valueType="mean")
reproCheck('4.10', boot_results$bounds[1], valueType="ci")
reproCheck('4.58', boot_results$bounds[2], valueType="ci")

boot_results = bootES(curious_data, R=10000, data.col="diff")
reproCheck('0.35', boot_results$t0, valueType="mean")
reproCheck('0.11', boot_results$bounds[1], valueType="ci")
reproCheck('0.59', boot_results$bounds[2], valueType="ci")

std_dev = sd(curious_data$T1)
reproCheck('1.32', std_dev, valueType="sd")
std_dev = sd(curious_data$T2)
reproCheck('1.25', std_dev, valueType="sd")

cohen_d = cohen.d(curious_data$T2,curious_data$T1)
reproCheck('0.27', cohen_d$estimate, valueType="d")

boot_results = bootES(interesting_data, R=10000, data.col="T1")
reproCheck('3.54', boot_results$t0, valueType="mean")
reproCheck('3.34', boot_results$bounds[1], valueType="ci")
reproCheck('3.73', boot_results$bounds[2], valueType="ci")

boot_results = bootES(interesting_data, R=10000, data.col="T2")
reproCheck('3.82', boot_results$t0, valueType="mean")
reproCheck('3.65', boot_results$bounds[1], valueType="ci")
reproCheck('4.00', boot_results$bounds[2], valueType="ci")

boot_results = bootES(interesting_data, R=10000, data.col="diff")
reproCheck('0.29', boot_results$t0, valueType="mean")
reproCheck('0.10', boot_results$bounds[1], valueType="ci")
reproCheck('0.47', boot_results$bounds[2], valueType="ci")

std_dev = sd(interesting_data$T1)
reproCheck('1.01', std_dev, valueType="sd")
std_dev = sd(interesting_data$T2)
reproCheck('0.89', std_dev, valueType="sd")

cohen_d = cohen.d(interesting_data$T2,interesting_data$T1)
reproCheck('0.29', cohen_d$estimate, valueType="d")

boot_results = bootES(surprising_data, R=10000, data.col="T1")
reproCheck('2.84', boot_results$t0, valueType="mean")
reproCheck('2.64', boot_results$bounds[1], valueType="ci")
reproCheck('3.05', boot_results$bounds[2], valueType="ci")

boot_results = bootES(surprising_data, R=10000, data.col="T2")
reproCheck('3.25', boot_results$t0, valueType="mean")
reproCheck('3.06', boot_results$bounds[1], valueType="ci")
reproCheck('3.44', boot_results$bounds[2], valueType="ci")

boot_results = bootES(surprising_data, R=10000, data.col="diff")
reproCheck('0.40', boot_results$t0, valueType="mean")
reproCheck('0.19', boot_results$bounds[1], valueType="ci")
reproCheck('0.62', boot_results$bounds[2], valueType="ci")

boot_results = bootES(meaningful_data, R=10000, data.col="T1")
reproCheck('3.81', boot_results$t0, valueType="mean")
reproCheck('3.60', boot_results$bounds[1], valueType="ci")
reproCheck('4.03', boot_results$bounds[2], valueType="ci")

boot_results = bootES(meaningful_data, R=10000, data.col="T2")
reproCheck('4.04', boot_results$t0, valueType="mean")
reproCheck('3.84', boot_results$bounds[1], valueType="ci")
reproCheck('4.23', boot_results$bounds[2], valueType="ci")

boot_results = bootES(meaningful_data, R=10000, data.col="diff")
reproCheck('0.22', boot_results$t0, valueType="mean")
reproCheck('0.03', boot_results$bounds[1], valueType="ci")
reproCheck('0.42', boot_results$bounds[2], valueType="ci")

boot_results = bootES(un_aggregated_interesting_data, R=10000, data.col="T1")
reproCheck('3.95', boot_results$t0, valueType="mean")
reproCheck('3.73', boot_results$bounds[1], valueType="ci")
reproCheck('4.18', boot_results$bounds[2], valueType="ci")

boot_results = bootES(un_aggregated_interesting_data, R=10000, data.col="T2")
reproCheck('4.19', boot_results$t0, valueType="mean")
reproCheck('4.00', boot_results$bounds[1], valueType="ci")
reproCheck('4.38', boot_results$bounds[2], valueType="ci")

boot_results = bootES(un_aggregated_interesting_data, R=10000, data.col="diff")
reproCheck('0.23', boot_results$t0, valueType="mean")
reproCheck('0.02', boot_results$bounds[1], valueType="ci")
reproCheck('0.45', boot_results$bounds[2], valueType="ci")

```

## Inferential statistics

For each of the 5 measures, we verify using a paired t-test that there is a significant difference between the distribution of the curiosity + interest measures at T1 and T2.

```{r}

t_test_results <- t.test(curious_data$T2,curious_data$T1,paired = T)
reproCheck(".005",t_test_results$p.value,valueType = 'p')
reproCheck("2.88", t_test_results$statistic, valueType = 't')

t_test_results <- t.test(interesting_data$T2,interesting_data$T1,paired = T)
reproCheck(".003",t_test_results$p.value,valueType = 'p')
reproCheck("3.10", t_test_results$statistic, valueType = 't')

t_test_results <- t.test(surprising_data$T2,surprising_data$T1,paired = T)
reproCheck("P < 0.001",t_test_results$p.value,valueType = 'p',eyeballCheck = T)

t_test_results <- t.test(meaningful_data$T2,meaningful_data$T1,paired = T)
reproCheck(".02",t_test_results$p.value,valueType = 'p')

t_test_results <- t.test(un_aggregated_interesting_data$T2,un_aggregated_interesting_data$T1,paired = T)
reproCheck(".03",t_test_results$p.value,valueType = 'p')
```

# Step 5: Conclusion

We consider the reproduction successful, even though there was one major error and several minor errors. The minor errors in the Bootstrapped 95% CIs are very small (about the same size as the variation between different runs, since the bootstrapping results in slightly different CI bounds each time the analysis is performed). 

There is also a small rounding error in the p value for the aggregated interest measure (The calculated value is 0.002477 and the reported value is 0.003). This doesn't result in a decision error, and in fact the rounded value is more conservative. It's easy to see how the authors might have rounded this incorrectly if SPSS reported the value as 0.0025.

The major error in Cohen's d reported in the pilot report was the result of a code error that was caught during co-piloting. There was a minor error, however (reported 0.29 vs. calculated 0.30) - we are not sure why this error might have occured.

The pilot added some explanatory comments to this document after the in-person co-piloting was finished.


```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR"))){
  finalOutcome <- "Failure"
}else{
  finalOutcome <- "Success"
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, finalOutcome)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "copilot"){
  write_csv(reportObject, "copilotReportDetailed.csv")
  write_csv(reportExtras, "copilotReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
